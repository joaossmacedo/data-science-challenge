{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d9c25f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab4ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3ef68",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37254b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('../datasets/classification_data/classification_train.csv')\n",
    "test_dataset = pd.read_csv('../datasets/classification_data/classification_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae7262",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f077b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ede51a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.070726</td>\n",
       "      <td>0.581557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.274437</td>\n",
       "      <td>-0.771944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.695878</td>\n",
       "      <td>1.019199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.891098</td>\n",
       "      <td>-0.529374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861008</td>\n",
       "      <td>0.519380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  target\n",
       "0 -0.070726  0.581557       0\n",
       "1  0.274437 -0.771944       1\n",
       "2 -0.695878  1.019199       0\n",
       "3  1.891098 -0.529374       1\n",
       "4  0.861008  0.519380       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d28286d",
   "metadata": {},
   "source": [
    "### Looking for outliers or inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628d0bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>670.000000</td>\n",
       "      <td>670.000000</td>\n",
       "      <td>670.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.481824</td>\n",
       "      <td>0.262611</td>\n",
       "      <td>0.505970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.914772</td>\n",
       "      <td>0.615023</td>\n",
       "      <td>0.500338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.939767</td>\n",
       "      <td>-1.313970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.159189</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.492489</td>\n",
       "      <td>0.275538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.159680</td>\n",
       "      <td>0.717589</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.590410</td>\n",
       "      <td>1.904169</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2      target\n",
       "count  670.000000  670.000000  670.000000\n",
       "mean     0.481824    0.262611    0.505970\n",
       "std      0.914772    0.615023    0.500338\n",
       "min     -1.939767   -1.313970    0.000000\n",
       "25%     -0.169615   -0.159189    0.000000\n",
       "50%      0.492489    0.275538    1.000000\n",
       "75%      1.159680    0.717589    1.000000\n",
       "max      2.590410    1.904169    1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab41f2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.472470</td>\n",
       "      <td>0.266104</td>\n",
       "      <td>0.487879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.986760</td>\n",
       "      <td>0.583819</td>\n",
       "      <td>0.500612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.693028</td>\n",
       "      <td>-1.031435</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.264152</td>\n",
       "      <td>-0.190844</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.503077</td>\n",
       "      <td>0.240346</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.218472</td>\n",
       "      <td>0.680558</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.528373</td>\n",
       "      <td>1.783693</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2      target\n",
       "count  330.000000  330.000000  330.000000\n",
       "mean     0.472470    0.266104    0.487879\n",
       "std      0.986760    0.583819    0.500612\n",
       "min     -1.693028   -1.031435    0.000000\n",
       "25%     -0.264152   -0.190844    0.000000\n",
       "50%      0.503077    0.240346    0.000000\n",
       "75%      1.218472    0.680558    1.000000\n",
       "max      2.528373    1.783693    1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc3093",
   "metadata": {},
   "source": [
    "### Split validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99908238",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(len(train_dataset) * 0.1)\n",
    "val_dataset = train_dataset.iloc[:split_size, :]\n",
    "train_dataset = train_dataset.iloc[split_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58885e33",
   "metadata": {},
   "source": [
    "### Setting dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f513e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(df, batch_size=10, shuffle=True):\n",
    "    target = torch.tensor(df['target'].values.astype(np.float32))\n",
    "    data = torch.tensor(df.drop('target', axis = 1).values.astype(np.float32)) \n",
    "    tensor = TensorDataset(data, target) \n",
    "    return DataLoader(dataset = tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214731e5",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc6d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layer_size=256):\n",
    "        super(Net, self).__init__()\n",
    "        self.hid1 = nn.Linear(2, hidden_layer_size)\n",
    "        self.hid2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.oupt = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hid1.weight) \n",
    "        nn.init.zeros_(self.hid1.bias)\n",
    "        nn.init.xavier_uniform_(self.hid2.weight) \n",
    "        nn.init.zeros_(self.hid2.bias)\n",
    "        nn.init.xavier_uniform_(self.oupt.weight)  \n",
    "        nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hid1(x)) \n",
    "        x = torch.tanh(self.hid2(x))\n",
    "        x = torch.sigmoid(self.oupt(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775a47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __log__(epoch, lr, train_loss, val_loss=None):\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    print(\"\\ttraining loss:    {}\".format(train_loss))\n",
    "    if val_loss is not None:\n",
    "        print(\"\\tvalidation loss : {}\".format(val_loss))\n",
    "    print(\"\\tlearning rate:    {}\".format(lr))\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.network = Net()\n",
    "    \n",
    "    def train(self, dataloader, val_dataloader=None, epochs=10, lr=0.001, log_epochs=10, patience=None):\n",
    "        # setup learning rate\n",
    "        # weight decay is an option so there must be 2 values\n",
    "        if isinstance(lr, (list, tuple)):\n",
    "            lr_bounds = lr[:2]\n",
    "        else:\n",
    "            lr_bounds = (lr, lr)\n",
    "        lr = lr_bounds[0]\n",
    "        \n",
    "        # calculate number of steps(dataset length * number of epochs)\n",
    "        steps = len(dataloader) * epochs\n",
    "        \n",
    "        # setup optimizer and loss function\n",
    "        optimizer = torch.optim.SGD(self.network.parameters(), lr=lr)\n",
    "        loss_function = nn.BCELoss()\n",
    "        \n",
    "        # keep loss so that it can be used to activate an early-stop mechanism(if patience is not None)\n",
    "        val_losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            running_val_loss = 0.0\n",
    "            \n",
    "            for x, y in dataloader:\n",
    "                # zero param gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # calculate output and loss\n",
    "                output = self.network(x)\n",
    "                loss = loss_function(output, y.reshape(-1,1))\n",
    "                \n",
    "                # back-propagate\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # weight decay\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] -= ((lr_bounds[0] - lr_bounds[1]) / steps)\n",
    "                    lr -= ((lr_bounds[0] - lr_bounds[1]) / steps)\n",
    "\n",
    "            if val_dataloader is None:\n",
    "                __log__(i, lr, running_loss / len(train_dataloader))\n",
    "                continue\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                for x, y in val_dataloader:\n",
    "                    # zero param gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # calculate output and loss\n",
    "                    output = self.network(x)\n",
    "                    running_val_loss += loss_function(output, y.reshape(-1,1))\n",
    "            __log__(i, lr, running_loss / len(train_dataloader), running_val_loss / len(val_dataloader))\n",
    "            \n",
    "            if patience is None:\n",
    "                continue\n",
    "                \n",
    "            val_losses.append(running_val_loss)\n",
    "\n",
    "            if len(val_losses) < patience + 1:\n",
    "                continue\n",
    "            val_losses = val_losses[-(patience + 1):]\n",
    "            if val_losses.index(min(val_losses)) == 0:\n",
    "                print('\\nEARLY STOP')\n",
    "                break\n",
    "\n",
    "\n",
    "    def predict(self, dataloader):        \n",
    "        predictions = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataloader:\n",
    "                # calculate output\n",
    "                output = self.network(x)\n",
    "                predicted = self.network(torch.tensor(x, dtype=torch.float32))\n",
    "\n",
    "                predictions += predicted.reshape(-1).detach().numpy().round().tolist()\n",
    "                labels += y\n",
    "\n",
    "        return predictions, labels\n",
    "    \n",
    "    def test(self, dataloader):\n",
    "        predictions, labels = self.predict(dataloader)\n",
    "    \n",
    "        counter = {\n",
    "            'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0\n",
    "        }\n",
    "                \n",
    "        for pred, lbl in zip(predictions, labels):\n",
    "            if pred == 1 and lbl == 1: res = 'tp'\n",
    "            elif pred == 1 and lbl == 0: res = 'fp'\n",
    "            elif pred == 0 and lbl == 0: res = 'tn'\n",
    "            else: res = 'fn'\n",
    "            counter[res] += 1\n",
    "\n",
    "        results = {\n",
    "            'precision': counter['tp'] / (counter['tp'] + counter['fp']),\n",
    "            'recall': counter['tp'] / (counter['tp'] + counter['fn'])\n",
    "        }\n",
    "        results['f1_score'] = \\\n",
    "            (2 * results['precision'] * results['recall']) / \\\n",
    "            (results['precision'] + results['recall'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, dir_path):\n",
    "        torch.save(\n",
    "            self.network.state_dict(),\n",
    "            os.path.join(dir_path, 'weights')\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a3748",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d40095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\ttraining loss:    0.5714566824866123\n",
      "\tvalidation loss : 0.48037999868392944\n",
      "\tlearning rate:    0.01\n",
      "epoch 1\n",
      "\ttraining loss:    0.4472302147110955\n",
      "\tvalidation loss : 0.40821829438209534\n",
      "\tlearning rate:    0.01\n",
      "epoch 2\n",
      "\ttraining loss:    0.40778477978510935\n",
      "\tvalidation loss : 0.38177207112312317\n",
      "\tlearning rate:    0.01\n",
      "epoch 3\n",
      "\ttraining loss:    0.3861849460689748\n",
      "\tvalidation loss : 0.3567824959754944\n",
      "\tlearning rate:    0.01\n",
      "epoch 4\n",
      "\ttraining loss:    0.38060415157529176\n",
      "\tvalidation loss : 0.3739399015903473\n",
      "\tlearning rate:    0.01\n",
      "epoch 5\n",
      "\ttraining loss:    0.3698070940912747\n",
      "\tvalidation loss : 0.34478244185447693\n",
      "\tlearning rate:    0.01\n",
      "epoch 6\n",
      "\ttraining loss:    0.36560543461656964\n",
      "\tvalidation loss : 0.3363526165485382\n",
      "\tlearning rate:    0.01\n",
      "epoch 7\n",
      "\ttraining loss:    0.36545764325095004\n",
      "\tvalidation loss : 0.34436944127082825\n",
      "\tlearning rate:    0.01\n",
      "epoch 8\n",
      "\ttraining loss:    0.36281279535567174\n",
      "\tvalidation loss : 0.33262521028518677\n",
      "\tlearning rate:    0.01\n",
      "epoch 9\n",
      "\ttraining loss:    0.36939205143783915\n",
      "\tvalidation loss : 0.33554959297180176\n",
      "\tlearning rate:    0.01\n",
      "epoch 10\n",
      "\ttraining loss:    0.36122808373365245\n",
      "\tvalidation loss : 0.3384706974029541\n",
      "\tlearning rate:    0.01\n",
      "epoch 11\n",
      "\ttraining loss:    0.3606204375868938\n",
      "\tvalidation loss : 0.32392698526382446\n",
      "\tlearning rate:    0.01\n",
      "epoch 12\n",
      "\ttraining loss:    0.35887037608467165\n",
      "\tvalidation loss : 0.3269125819206238\n",
      "\tlearning rate:    0.01\n",
      "epoch 13\n",
      "\ttraining loss:    0.3569467559334685\n",
      "\tvalidation loss : 0.3525591194629669\n",
      "\tlearning rate:    0.01\n",
      "epoch 14\n",
      "\ttraining loss:    0.3624274729460966\n",
      "\tvalidation loss : 0.33408403396606445\n",
      "\tlearning rate:    0.01\n",
      "epoch 15\n",
      "\ttraining loss:    0.35640792512014263\n",
      "\tvalidation loss : 0.3248934745788574\n",
      "\tlearning rate:    0.01\n",
      "epoch 16\n",
      "\ttraining loss:    0.35732745293711055\n",
      "\tvalidation loss : 0.3248054087162018\n",
      "\tlearning rate:    0.01\n",
      "epoch 17\n",
      "\ttraining loss:    0.35720397776267565\n",
      "\tvalidation loss : 0.3241889476776123\n",
      "\tlearning rate:    0.01\n",
      "epoch 18\n",
      "\ttraining loss:    0.35719277941789784\n",
      "\tvalidation loss : 0.3265700340270996\n",
      "\tlearning rate:    0.01\n",
      "epoch 19\n",
      "\ttraining loss:    0.356002323024097\n",
      "\tvalidation loss : 0.36087724566459656\n",
      "\tlearning rate:    0.01\n",
      "epoch 20\n",
      "\ttraining loss:    0.3564151657409355\n",
      "\tvalidation loss : 0.325327605009079\n",
      "\tlearning rate:    0.01\n",
      "epoch 21\n",
      "\ttraining loss:    0.3574528425443368\n",
      "\tvalidation loss : 0.32297995686531067\n",
      "\tlearning rate:    0.01\n",
      "epoch 22\n",
      "\ttraining loss:    0.3574597472782995\n",
      "\tvalidation loss : 0.3363044559955597\n",
      "\tlearning rate:    0.01\n",
      "epoch 23\n",
      "\ttraining loss:    0.36539744878890085\n",
      "\tvalidation loss : 0.32998520135879517\n",
      "\tlearning rate:    0.01\n",
      "epoch 24\n",
      "\ttraining loss:    0.3555067055476982\n",
      "\tvalidation loss : 0.3303459584712982\n",
      "\tlearning rate:    0.01\n",
      "epoch 25\n",
      "\ttraining loss:    0.3627092981191932\n",
      "\tvalidation loss : 0.34326738119125366\n",
      "\tlearning rate:    0.01\n",
      "epoch 26\n",
      "\ttraining loss:    0.36007222313372816\n",
      "\tvalidation loss : 0.328016459941864\n",
      "\tlearning rate:    0.01\n",
      "epoch 27\n",
      "\ttraining loss:    0.35766838698602116\n",
      "\tvalidation loss : 0.32567712664604187\n",
      "\tlearning rate:    0.01\n",
      "epoch 28\n",
      "\ttraining loss:    0.3630952846075668\n",
      "\tvalidation loss : 0.32809799909591675\n",
      "\tlearning rate:    0.01\n",
      "epoch 29\n",
      "\ttraining loss:    0.3570914588502196\n",
      "\tvalidation loss : 0.3272245526313782\n",
      "\tlearning rate:    0.01\n",
      "epoch 30\n",
      "\ttraining loss:    0.3581689042268229\n",
      "\tvalidation loss : 0.353383332490921\n",
      "\tlearning rate:    0.01\n",
      "epoch 31\n",
      "\ttraining loss:    0.3587859457999956\n",
      "\tvalidation loss : 0.33109885454177856\n",
      "\tlearning rate:    0.01\n",
      "epoch 32\n",
      "\ttraining loss:    0.35612327642128117\n",
      "\tvalidation loss : 0.32236459851264954\n",
      "\tlearning rate:    0.01\n",
      "epoch 33\n",
      "\ttraining loss:    0.35824385028882105\n",
      "\tvalidation loss : 0.3235141336917877\n",
      "\tlearning rate:    0.01\n",
      "epoch 34\n",
      "\ttraining loss:    0.36022341422370224\n",
      "\tvalidation loss : 0.32384559512138367\n",
      "\tlearning rate:    0.01\n",
      "epoch 35\n",
      "\ttraining loss:    0.3603794153352253\n",
      "\tvalidation loss : 0.33598408102989197\n",
      "\tlearning rate:    0.01\n",
      "epoch 36\n",
      "\ttraining loss:    0.35823333092400284\n",
      "\tvalidation loss : 0.3307678699493408\n",
      "\tlearning rate:    0.01\n",
      "epoch 37\n",
      "\ttraining loss:    0.3605118650393408\n",
      "\tvalidation loss : 0.3297004699707031\n",
      "\tlearning rate:    0.01\n",
      "epoch 38\n",
      "\ttraining loss:    0.35752566170985584\n",
      "\tvalidation loss : 0.32296067476272583\n",
      "\tlearning rate:    0.01\n",
      "epoch 39\n",
      "\ttraining loss:    0.36112068275936315\n",
      "\tvalidation loss : 0.3301764130592346\n",
      "\tlearning rate:    0.01\n",
      "epoch 40\n",
      "\ttraining loss:    0.35797912444247576\n",
      "\tvalidation loss : 0.3302292227745056\n",
      "\tlearning rate:    0.01\n",
      "epoch 41\n",
      "\ttraining loss:    0.35760273906539697\n",
      "\tvalidation loss : 0.33243775367736816\n",
      "\tlearning rate:    0.01\n",
      "epoch 42\n",
      "\ttraining loss:    0.3603933730819186\n",
      "\tvalidation loss : 0.3438759744167328\n",
      "\tlearning rate:    0.01\n",
      "epoch 43\n",
      "\ttraining loss:    0.35616887360811234\n",
      "\tvalidation loss : 0.34407949447631836\n",
      "\tlearning rate:    0.01\n",
      "epoch 44\n",
      "\ttraining loss:    0.35573826443220746\n",
      "\tvalidation loss : 0.3275337219238281\n",
      "\tlearning rate:    0.01\n",
      "epoch 45\n",
      "\ttraining loss:    0.3575471959641722\n",
      "\tvalidation loss : 0.32156091928482056\n",
      "\tlearning rate:    0.01\n",
      "epoch 46\n",
      "\ttraining loss:    0.3587772361323482\n",
      "\tvalidation loss : 0.35172420740127563\n",
      "\tlearning rate:    0.01\n",
      "epoch 47\n",
      "\ttraining loss:    0.3596026295276939\n",
      "\tvalidation loss : 0.3325916826725006\n",
      "\tlearning rate:    0.01\n",
      "epoch 48\n",
      "\ttraining loss:    0.3567162954904994\n",
      "\tvalidation loss : 0.32970085740089417\n",
      "\tlearning rate:    0.01\n",
      "epoch 49\n",
      "\ttraining loss:    0.3585496899045882\n",
      "\tvalidation loss : 0.32537028193473816\n",
      "\tlearning rate:    0.01\n",
      "epoch 50\n",
      "\ttraining loss:    0.3554373672995411\n",
      "\tvalidation loss : 0.33350101113319397\n",
      "\tlearning rate:    0.01\n",
      "epoch 51\n",
      "\ttraining loss:    0.3555119465120503\n",
      "\tvalidation loss : 0.32298025488853455\n",
      "\tlearning rate:    0.01\n",
      "epoch 52\n",
      "\ttraining loss:    0.3584004387748046\n",
      "\tvalidation loss : 0.3351023197174072\n",
      "\tlearning rate:    0.01\n",
      "epoch 53\n",
      "\ttraining loss:    0.35734822723220605\n",
      "\tvalidation loss : 0.35078877210617065\n",
      "\tlearning rate:    0.01\n",
      "epoch 54\n",
      "\ttraining loss:    0.3579043362472878\n",
      "\tvalidation loss : 0.32799115777015686\n",
      "\tlearning rate:    0.01\n",
      "epoch 55\n",
      "\ttraining loss:    0.35690910210374927\n",
      "\tvalidation loss : 0.33522698283195496\n",
      "\tlearning rate:    0.01\n",
      "epoch 56\n",
      "\ttraining loss:    0.35681843501134\n",
      "\tvalidation loss : 0.3221621513366699\n",
      "\tlearning rate:    0.01\n",
      "epoch 57\n",
      "\ttraining loss:    0.3558766321569193\n",
      "\tvalidation loss : 0.3242897093296051\n",
      "\tlearning rate:    0.01\n",
      "epoch 58\n",
      "\ttraining loss:    0.3696709828542881\n",
      "\tvalidation loss : 0.32194608449935913\n",
      "\tlearning rate:    0.01\n",
      "epoch 59\n",
      "\ttraining loss:    0.3584938418181216\n",
      "\tvalidation loss : 0.3232572078704834\n",
      "\tlearning rate:    0.01\n",
      "epoch 60\n",
      "\ttraining loss:    0.3627315581333442\n",
      "\tvalidation loss : 0.34053704142570496\n",
      "\tlearning rate:    0.01\n",
      "epoch 61\n",
      "\ttraining loss:    0.3563113316405015\n",
      "\tvalidation loss : 0.3204488158226013\n",
      "\tlearning rate:    0.01\n",
      "epoch 62\n",
      "\ttraining loss:    0.357960647124736\n",
      "\tvalidation loss : 0.32505011558532715\n",
      "\tlearning rate:    0.01\n",
      "epoch 63\n",
      "\ttraining loss:    0.36018658185102903\n",
      "\tvalidation loss : 0.32392793893814087\n",
      "\tlearning rate:    0.01\n",
      "epoch 64\n",
      "\ttraining loss:    0.36068767748895236\n",
      "\tvalidation loss : 0.32672110199928284\n",
      "\tlearning rate:    0.01\n",
      "epoch 65\n",
      "\ttraining loss:    0.35593398831418305\n",
      "\tvalidation loss : 0.3233483135700226\n",
      "\tlearning rate:    0.01\n",
      "epoch 66\n",
      "\ttraining loss:    0.35598078919727294\n",
      "\tvalidation loss : 0.33390066027641296\n",
      "\tlearning rate:    0.01\n",
      "epoch 67\n",
      "\ttraining loss:    0.35798320491782953\n",
      "\tvalidation loss : 0.34122952818870544\n",
      "\tlearning rate:    0.01\n",
      "epoch 68\n",
      "\ttraining loss:    0.36068580775964454\n",
      "\tvalidation loss : 0.3413175940513611\n",
      "\tlearning rate:    0.01\n",
      "epoch 69\n",
      "\ttraining loss:    0.36678429986121225\n",
      "\tvalidation loss : 0.329241544008255\n",
      "\tlearning rate:    0.01\n",
      "epoch 70\n",
      "\ttraining loss:    0.3616143334351602\n",
      "\tvalidation loss : 0.32433733344078064\n",
      "\tlearning rate:    0.01\n",
      "epoch 71\n",
      "\ttraining loss:    0.35723670217834536\n",
      "\tvalidation loss : 0.3311225473880768\n",
      "\tlearning rate:    0.01\n",
      "epoch 72\n",
      "\ttraining loss:    0.3566952991436739\n",
      "\tvalidation loss : 0.34155192971229553\n",
      "\tlearning rate:    0.01\n",
      "epoch 73\n",
      "\ttraining loss:    0.3612223386031682\n",
      "\tvalidation loss : 0.3313441872596741\n",
      "\tlearning rate:    0.01\n",
      "epoch 74\n",
      "\ttraining loss:    0.36360926865065685\n",
      "\tvalidation loss : 0.3538462221622467\n",
      "\tlearning rate:    0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75\n",
      "\ttraining loss:    0.360462555753403\n",
      "\tvalidation loss : 0.34527334570884705\n",
      "\tlearning rate:    0.01\n",
      "epoch 76\n",
      "\ttraining loss:    0.3598226620281329\n",
      "\tvalidation loss : 0.32853227853775024\n",
      "\tlearning rate:    0.01\n",
      "epoch 77\n",
      "\ttraining loss:    0.3557408431880787\n",
      "\tvalidation loss : 0.3282530605792999\n",
      "\tlearning rate:    0.01\n",
      "epoch 78\n",
      "\ttraining loss:    0.356895456915019\n",
      "\tvalidation loss : 0.3443169593811035\n",
      "\tlearning rate:    0.01\n",
      "epoch 79\n",
      "\ttraining loss:    0.3565399064148059\n",
      "\tvalidation loss : 0.33597514033317566\n",
      "\tlearning rate:    0.01\n",
      "epoch 80\n",
      "\ttraining loss:    0.35740379915862786\n",
      "\tvalidation loss : 0.3450572192668915\n",
      "\tlearning rate:    0.01\n",
      "epoch 81\n",
      "\ttraining loss:    0.3582492506162065\n",
      "\tvalidation loss : 0.3414069712162018\n",
      "\tlearning rate:    0.01\n",
      "\n",
      "EARLY STOP\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "hidden_layer_size = 256\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "log_epochs = 1\n",
    "\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = Model()\n",
    "model.train(\n",
    "    train_dataloader, val_dataloader=val_dataloader,\n",
    "    epochs=epochs, lr=learning_rate, log_epochs=log_epochs,\n",
    "    patience=int(epochs*0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66fda72",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a65a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaomacedo/opt/anaconda3/envs/oncase/lib/python3.7/site-packages/ipykernel_launcher.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8165680473372781,\n",
       " 'recall': 0.8571428571428571,\n",
       " 'f1_score': 0.8363636363636363}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.test(test_dataloader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae03b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('..', 'models', 'classification')\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "model.save(path)\n",
    "\n",
    "with open(os.path.join(path, 'results.json'), 'w') as f:\n",
    "    f.write(json.dumps(results, indent=4, ensure_ascii=False))\n",
    "\n",
    "with open(os.path.join(path, 'train_info.json'), 'w') as f:\n",
    "    f.write(json.dumps({\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'hidden_layer_size': hidden_layer_size\n",
    "    }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
