{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d9c25f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa7388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3ef68",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37254b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('../datasets/classification_data/classification_train.csv')\n",
    "test_dataset = pd.read_csv('../datasets/classification_data/classification_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8eb49",
   "metadata": {},
   "source": [
    "### Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eccb48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "test_dataset = test_dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b796622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.194505</td>\n",
       "      <td>-0.498167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.870737</td>\n",
       "      <td>-0.062688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307560</td>\n",
       "      <td>0.798304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521713</td>\n",
       "      <td>0.318790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.207414</td>\n",
       "      <td>-1.135872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  target\n",
       "0  1.194505 -0.498167       1\n",
       "1 -0.870737 -0.062688       0\n",
       "2  0.307560  0.798304       0\n",
       "3  0.521713  0.318790       0\n",
       "4  1.207414 -1.135872       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41450de0",
   "metadata": {},
   "source": [
    "### Looking for outliers or inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889b8167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>670.000000</td>\n",
       "      <td>670.000000</td>\n",
       "      <td>670.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.481824</td>\n",
       "      <td>0.262611</td>\n",
       "      <td>0.505970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.914772</td>\n",
       "      <td>0.615023</td>\n",
       "      <td>0.500338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.939767</td>\n",
       "      <td>-1.313970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.159189</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.492489</td>\n",
       "      <td>0.275538</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.159680</td>\n",
       "      <td>0.717589</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.590410</td>\n",
       "      <td>1.904169</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2      target\n",
       "count  670.000000  670.000000  670.000000\n",
       "mean     0.481824    0.262611    0.505970\n",
       "std      0.914772    0.615023    0.500338\n",
       "min     -1.939767   -1.313970    0.000000\n",
       "25%     -0.169615   -0.159189    0.000000\n",
       "50%      0.492489    0.275538    1.000000\n",
       "75%      1.159680    0.717589    1.000000\n",
       "max      2.590410    1.904169    1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f677c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>330.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.472470</td>\n",
       "      <td>0.266104</td>\n",
       "      <td>0.487879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.986760</td>\n",
       "      <td>0.583819</td>\n",
       "      <td>0.500612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.693028</td>\n",
       "      <td>-1.031435</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.264152</td>\n",
       "      <td>-0.190844</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.503077</td>\n",
       "      <td>0.240346</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.218472</td>\n",
       "      <td>0.680558</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.528373</td>\n",
       "      <td>1.783693</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2      target\n",
       "count  330.000000  330.000000  330.000000\n",
       "mean     0.472470    0.266104    0.487879\n",
       "std      0.986760    0.583819    0.500612\n",
       "min     -1.693028   -1.031435    0.000000\n",
       "25%     -0.264152   -0.190844    0.000000\n",
       "50%      0.503077    0.240346    0.000000\n",
       "75%      1.218472    0.680558    1.000000\n",
       "max      2.528373    1.783693    1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700d268",
   "metadata": {},
   "source": [
    "### Split validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92842684",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(len(train_dataset) * 0.1)\n",
    "val_dataset = train_dataset.iloc[:split_size, :]\n",
    "train_dataset = train_dataset.iloc[split_size:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41db10",
   "metadata": {},
   "source": [
    "### Min Max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8fe124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_dataset)\n",
    "train_dataset = pd.DataFrame(scaler.transform(train_dataset), columns=train_dataset.columns)\n",
    "val_dataset = pd.DataFrame(scaler.transform(val_dataset), columns=val_dataset.columns)\n",
    "test_dataset = pd.DataFrame(scaler.transform(test_dataset), columns=test_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890e13f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>603.000000</td>\n",
       "      <td>603.000000</td>\n",
       "      <td>603.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.536186</td>\n",
       "      <td>0.515149</td>\n",
       "      <td>0.510779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.201184</td>\n",
       "      <td>0.200293</td>\n",
       "      <td>0.500299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.394438</td>\n",
       "      <td>0.376372</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.537185</td>\n",
       "      <td>0.520101</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.685011</td>\n",
       "      <td>0.661610</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               x1          x2      target\n",
       "count  603.000000  603.000000  603.000000\n",
       "mean     0.536186    0.515149    0.510779\n",
       "std      0.201184    0.200293    0.500299\n",
       "min      0.000000    0.000000    0.000000\n",
       "25%      0.394438    0.376372    0.000000\n",
       "50%      0.537185    0.520101    1.000000\n",
       "75%      0.685011    0.661610    1.000000\n",
       "max      1.000000    1.000000    1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b2ed6",
   "metadata": {},
   "source": [
    "### Setting dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d991c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(df, batch_size=10, shuffle=True):\n",
    "    target = torch.tensor(df['target'].values.astype(np.float32))\n",
    "    data = torch.tensor(df.drop('target', axis = 1).values.astype(np.float32)) \n",
    "    tensor = TensorDataset(data, target) \n",
    "    return DataLoader(dataset = tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb961",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65b6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layer_size=256):\n",
    "        super(Net, self).__init__()\n",
    "        self.hid1 = nn.Linear(2, hidden_layer_size)\n",
    "        self.hid2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.oupt = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hid1.weight) \n",
    "        nn.init.zeros_(self.hid1.bias)\n",
    "        nn.init.xavier_uniform_(self.hid2.weight) \n",
    "        nn.init.zeros_(self.hid2.bias)\n",
    "        nn.init.xavier_uniform_(self.oupt.weight)  \n",
    "        nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hid1(x)) \n",
    "        x = torch.tanh(self.hid2(x))\n",
    "        x = torch.sigmoid(self.oupt(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa8e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __log__(epoch, lr, train_loss, val_loss=None):\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    print(\"\\ttraining loss:    {}\".format(train_loss))\n",
    "    if val_loss is not None:\n",
    "        print(\"\\tvalidation loss : {}\".format(val_loss))\n",
    "    print(\"\\tlearning rate:    {}\".format(lr))\n",
    "\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.network = Net()\n",
    "    \n",
    "    def train(self, dataloader, val_dataloader=None, epochs=10, lr=0.001, log_epochs=10, patience=None):\n",
    "        # setup learning rate\n",
    "        # weight decay is an option so there must be 2 values\n",
    "        if isinstance(lr, (list, tuple)):\n",
    "            lr_bounds = lr[:2]\n",
    "        else:\n",
    "            lr_bounds = (lr, lr)\n",
    "        lr = lr_bounds[0]\n",
    "        \n",
    "        # calculate number of steps(dataset length * number of epochs)\n",
    "        steps = len(dataloader) * epochs\n",
    "        \n",
    "        # setup optimizer and loss function\n",
    "        optimizer = torch.optim.SGD(self.network.parameters(), lr=lr)\n",
    "        loss_function = nn.BCELoss()\n",
    "        \n",
    "        # keep loss so that it can be used to activate an early-stop mechanism(if patience is not None)\n",
    "        val_losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            running_val_loss = 0.0\n",
    "            \n",
    "            for x, y in dataloader:\n",
    "                # zero param gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # calculate output and loss\n",
    "                output = self.network(x)\n",
    "                loss = loss_function(output, y.reshape(-1,1))\n",
    "                \n",
    "                # back-propagate\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                # weight decay\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] -= ((lr_bounds[0] - lr_bounds[1]) / steps)\n",
    "                    lr -= ((lr_bounds[0] - lr_bounds[1]) / steps)\n",
    "\n",
    "            if val_dataloader is None:\n",
    "                __log__(i, lr, running_loss / len(train_dataloader))\n",
    "                continue\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                for x, y in val_dataloader:\n",
    "                    # zero param gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # calculate output and loss\n",
    "                    output = self.network(x)\n",
    "                    running_val_loss += loss_function(output, y.reshape(-1,1))\n",
    "            __log__(i, lr, running_loss / len(train_dataloader), running_val_loss / len(val_dataloader))\n",
    "            \n",
    "            if patience is None:\n",
    "                continue\n",
    "                \n",
    "            val_losses.append(running_val_loss)\n",
    "\n",
    "            if len(val_losses) < patience + 1:\n",
    "                continue\n",
    "            val_losses = val_losses[-(patience + 1):]\n",
    "            if val_losses.index(min(val_losses)) == 0:\n",
    "                print('\\nEARLY STOP')\n",
    "                break\n",
    "\n",
    "\n",
    "    def predict(self, dataloader):        \n",
    "        predictions = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataloader:\n",
    "                # calculate output\n",
    "                output = self.network(x)\n",
    "                predicted = self.network(torch.tensor(x, dtype=torch.float32))\n",
    "\n",
    "                predictions += predicted.reshape(-1).detach().numpy().round().tolist()\n",
    "                labels += y\n",
    "\n",
    "        return predictions, labels\n",
    "    \n",
    "    def test(self, dataloader):\n",
    "        predictions, labels = self.predict(dataloader)\n",
    "    \n",
    "        counter = {\n",
    "            'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0\n",
    "        }\n",
    "                \n",
    "        for pred, lbl in zip(predictions, labels):\n",
    "            if pred == 1 and lbl == 1: res = 'tp'\n",
    "            elif pred == 1 and lbl == 0: res = 'fp'\n",
    "            elif pred == 0 and lbl == 0: res = 'tn'\n",
    "            else: res = 'fn'\n",
    "            counter[res] += 1\n",
    "\n",
    "        results = {\n",
    "            'precision': counter['tp'] / (counter['tp'] + counter['fp']),\n",
    "            'recall': counter['tp'] / (counter['tp'] + counter['fn'])\n",
    "        }\n",
    "        results['f1_score'] = \\\n",
    "            (2 * results['precision'] * results['recall']) / \\\n",
    "            (results['precision'] + results['recall'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, dir_path):\n",
    "        torch.save(\n",
    "            self.network.state_dict(),\n",
    "            os.path.join(dir_path, 'weights')\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a3748",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76946939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\ttraining loss:    0.6800158649194435\n",
      "\tvalidation loss : 0.6575518846511841\n",
      "\tlearning rate:    0.01\n",
      "epoch 1\n",
      "\ttraining loss:    0.648031343202122\n",
      "\tvalidation loss : 0.6246320009231567\n",
      "\tlearning rate:    0.01\n",
      "epoch 2\n",
      "\ttraining loss:    0.6194437312298133\n",
      "\tvalidation loss : 0.5957816243171692\n",
      "\tlearning rate:    0.01\n",
      "epoch 3\n",
      "\ttraining loss:    0.5913253490064965\n",
      "\tvalidation loss : 0.5625170469284058\n",
      "\tlearning rate:    0.01\n",
      "epoch 4\n",
      "\ttraining loss:    0.5643074703021128\n",
      "\tvalidation loss : 0.5316867232322693\n",
      "\tlearning rate:    0.01\n",
      "epoch 5\n",
      "\ttraining loss:    0.5390741004318488\n",
      "\tvalidation loss : 0.5026623606681824\n",
      "\tlearning rate:    0.01\n",
      "epoch 6\n",
      "\ttraining loss:    0.5180547813900181\n",
      "\tvalidation loss : 0.4797614514827728\n",
      "\tlearning rate:    0.01\n",
      "epoch 7\n",
      "\ttraining loss:    0.49343893371644565\n",
      "\tvalidation loss : 0.45265477895736694\n",
      "\tlearning rate:    0.01\n",
      "epoch 8\n",
      "\ttraining loss:    0.47221136288564713\n",
      "\tvalidation loss : 0.42917531728744507\n",
      "\tlearning rate:    0.01\n",
      "epoch 9\n",
      "\ttraining loss:    0.4621533492549521\n",
      "\tvalidation loss : 0.41291338205337524\n",
      "\tlearning rate:    0.01\n",
      "epoch 10\n",
      "\ttraining loss:    0.439949563047925\n",
      "\tvalidation loss : 0.4063795506954193\n",
      "\tlearning rate:    0.01\n",
      "epoch 11\n",
      "\ttraining loss:    0.4269102789828035\n",
      "\tvalidation loss : 0.38689255714416504\n",
      "\tlearning rate:    0.01\n",
      "epoch 12\n",
      "\ttraining loss:    0.41415068629335183\n",
      "\tvalidation loss : 0.368259996175766\n",
      "\tlearning rate:    0.01\n",
      "epoch 13\n",
      "\ttraining loss:    0.4130454837787347\n",
      "\tvalidation loss : 0.36062291264533997\n",
      "\tlearning rate:    0.01\n",
      "epoch 14\n",
      "\ttraining loss:    0.3984969831148132\n",
      "\tvalidation loss : 0.3620395362377167\n",
      "\tlearning rate:    0.01\n",
      "epoch 15\n",
      "\ttraining loss:    0.39724106280530086\n",
      "\tvalidation loss : 0.346736341714859\n",
      "\tlearning rate:    0.01\n",
      "epoch 16\n",
      "\ttraining loss:    0.3879342372300195\n",
      "\tvalidation loss : 0.34302499890327454\n",
      "\tlearning rate:    0.01\n",
      "epoch 17\n",
      "\ttraining loss:    0.3831137809597078\n",
      "\tvalidation loss : 0.34752631187438965\n",
      "\tlearning rate:    0.01\n",
      "epoch 18\n",
      "\ttraining loss:    0.38418179384020507\n",
      "\tvalidation loss : 0.33421340584754944\n",
      "\tlearning rate:    0.01\n",
      "epoch 19\n",
      "\ttraining loss:    0.3768192602962744\n",
      "\tvalidation loss : 0.33577075600624084\n",
      "\tlearning rate:    0.01\n",
      "epoch 20\n",
      "\ttraining loss:    0.384531812467536\n",
      "\tvalidation loss : 0.32144707441329956\n",
      "\tlearning rate:    0.01\n",
      "epoch 21\n",
      "\ttraining loss:    0.3737746887519711\n",
      "\tvalidation loss : 0.3167647421360016\n",
      "\tlearning rate:    0.01\n",
      "epoch 22\n",
      "\ttraining loss:    0.3740386101798933\n",
      "\tvalidation loss : 0.32271772623062134\n",
      "\tlearning rate:    0.01\n",
      "epoch 23\n",
      "\ttraining loss:    0.37188125010885176\n",
      "\tvalidation loss : 0.3312141001224518\n",
      "\tlearning rate:    0.01\n",
      "epoch 24\n",
      "\ttraining loss:    0.3705976407547466\n",
      "\tvalidation loss : 0.31467631459236145\n",
      "\tlearning rate:    0.01\n",
      "epoch 25\n",
      "\ttraining loss:    0.3704088631956304\n",
      "\tvalidation loss : 0.3255816102027893\n",
      "\tlearning rate:    0.01\n",
      "epoch 26\n",
      "\ttraining loss:    0.3733373120671413\n",
      "\tvalidation loss : 0.3305588662624359\n",
      "\tlearning rate:    0.01\n",
      "epoch 27\n",
      "\ttraining loss:    0.3718028544891076\n",
      "\tvalidation loss : 0.3100575804710388\n",
      "\tlearning rate:    0.01\n",
      "epoch 28\n",
      "\ttraining loss:    0.3703091283802126\n",
      "\tvalidation loss : 0.31900709867477417\n",
      "\tlearning rate:    0.01\n",
      "epoch 29\n",
      "\ttraining loss:    0.37049818234365495\n",
      "\tvalidation loss : 0.33038610219955444\n",
      "\tlearning rate:    0.01\n",
      "epoch 30\n",
      "\ttraining loss:    0.36649977476870427\n",
      "\tvalidation loss : 0.3215169608592987\n",
      "\tlearning rate:    0.01\n",
      "epoch 31\n",
      "\ttraining loss:    0.3651469691855008\n",
      "\tvalidation loss : 0.3295491337776184\n",
      "\tlearning rate:    0.01\n",
      "epoch 32\n",
      "\ttraining loss:    0.3671069820640517\n",
      "\tvalidation loss : 0.3407553732395172\n",
      "\tlearning rate:    0.01\n",
      "epoch 33\n",
      "\ttraining loss:    0.3616239407756289\n",
      "\tvalidation loss : 0.3090461194515228\n",
      "\tlearning rate:    0.01\n",
      "epoch 34\n",
      "\ttraining loss:    0.3633405485602676\n",
      "\tvalidation loss : 0.30833548307418823\n",
      "\tlearning rate:    0.01\n",
      "epoch 35\n",
      "\ttraining loss:    0.3705081605031842\n",
      "\tvalidation loss : 0.33560124039649963\n",
      "\tlearning rate:    0.01\n",
      "epoch 36\n",
      "\ttraining loss:    0.3673235544415771\n",
      "\tvalidation loss : 0.3269163966178894\n",
      "\tlearning rate:    0.01\n",
      "epoch 37\n",
      "\ttraining loss:    0.3634038342804205\n",
      "\tvalidation loss : 0.30497121810913086\n",
      "\tlearning rate:    0.01\n",
      "epoch 38\n",
      "\ttraining loss:    0.36468249058625735\n",
      "\tvalidation loss : 0.3129940629005432\n",
      "\tlearning rate:    0.01\n",
      "epoch 39\n",
      "\ttraining loss:    0.36160620949307426\n",
      "\tvalidation loss : 0.3104693293571472\n",
      "\tlearning rate:    0.01\n",
      "epoch 40\n",
      "\ttraining loss:    0.36454723079185014\n",
      "\tvalidation loss : 0.32062405347824097\n",
      "\tlearning rate:    0.01\n",
      "epoch 41\n",
      "\ttraining loss:    0.3621152263195788\n",
      "\tvalidation loss : 0.3156348764896393\n",
      "\tlearning rate:    0.01\n",
      "epoch 42\n",
      "\ttraining loss:    0.36350600238217684\n",
      "\tvalidation loss : 0.3226936161518097\n",
      "\tlearning rate:    0.01\n",
      "epoch 43\n",
      "\ttraining loss:    0.3695163365880974\n",
      "\tvalidation loss : 0.32678738236427307\n",
      "\tlearning rate:    0.01\n",
      "epoch 44\n",
      "\ttraining loss:    0.3623603168325346\n",
      "\tvalidation loss : 0.3020721971988678\n",
      "\tlearning rate:    0.01\n",
      "epoch 45\n",
      "\ttraining loss:    0.3799781860386739\n",
      "\tvalidation loss : 0.3163203299045563\n",
      "\tlearning rate:    0.01\n",
      "epoch 46\n",
      "\ttraining loss:    0.3599220148486192\n",
      "\tvalidation loss : 0.31828346848487854\n",
      "\tlearning rate:    0.01\n",
      "epoch 47\n",
      "\ttraining loss:    0.362638209015131\n",
      "\tvalidation loss : 0.3365986943244934\n",
      "\tlearning rate:    0.01\n",
      "epoch 48\n",
      "\ttraining loss:    0.360893652209493\n",
      "\tvalidation loss : 0.3102376163005829\n",
      "\tlearning rate:    0.01\n",
      "epoch 49\n",
      "\ttraining loss:    0.36766349108981305\n",
      "\tvalidation loss : 0.335520476102829\n",
      "\tlearning rate:    0.01\n",
      "epoch 50\n",
      "\ttraining loss:    0.3673795734028347\n",
      "\tvalidation loss : 0.30213668942451477\n",
      "\tlearning rate:    0.01\n",
      "epoch 51\n",
      "\ttraining loss:    0.3606250349615441\n",
      "\tvalidation loss : 0.3126828372478485\n",
      "\tlearning rate:    0.01\n",
      "epoch 52\n",
      "\ttraining loss:    0.3774392830788112\n",
      "\tvalidation loss : 0.31623488664627075\n",
      "\tlearning rate:    0.01\n",
      "epoch 53\n",
      "\ttraining loss:    0.35925800127328417\n",
      "\tvalidation loss : 0.317501962184906\n",
      "\tlearning rate:    0.01\n",
      "epoch 54\n",
      "\ttraining loss:    0.36105689552963754\n",
      "\tvalidation loss : 0.30676698684692383\n",
      "\tlearning rate:    0.01\n",
      "epoch 55\n",
      "\ttraining loss:    0.3608153650506598\n",
      "\tvalidation loss : 0.32183268666267395\n",
      "\tlearning rate:    0.01\n",
      "epoch 56\n",
      "\ttraining loss:    0.3601085643787853\n",
      "\tvalidation loss : 0.30176061391830444\n",
      "\tlearning rate:    0.01\n",
      "epoch 57\n",
      "\ttraining loss:    0.3607117511698457\n",
      "\tvalidation loss : 0.3259718716144562\n",
      "\tlearning rate:    0.01\n",
      "epoch 58\n",
      "\ttraining loss:    0.3586849343092715\n",
      "\tvalidation loss : 0.33747100830078125\n",
      "\tlearning rate:    0.01\n",
      "epoch 59\n",
      "\ttraining loss:    0.3605289770687213\n",
      "\tvalidation loss : 0.3128102719783783\n",
      "\tlearning rate:    0.01\n",
      "epoch 60\n",
      "\ttraining loss:    0.3591664669821497\n",
      "\tvalidation loss : 0.3209278881549835\n",
      "\tlearning rate:    0.01\n",
      "epoch 61\n",
      "\ttraining loss:    0.357915679450895\n",
      "\tvalidation loss : 0.3037375807762146\n",
      "\tlearning rate:    0.01\n",
      "epoch 62\n",
      "\ttraining loss:    0.3614787945493323\n",
      "\tvalidation loss : 0.31701382994651794\n",
      "\tlearning rate:    0.01\n",
      "epoch 63\n",
      "\ttraining loss:    0.3678460905297858\n",
      "\tvalidation loss : 0.2980829179286957\n",
      "\tlearning rate:    0.01\n",
      "epoch 64\n",
      "\ttraining loss:    0.3657597029795412\n",
      "\tvalidation loss : 0.309500128030777\n",
      "\tlearning rate:    0.01\n",
      "epoch 65\n",
      "\ttraining loss:    0.3644005110273596\n",
      "\tvalidation loss : 0.3207072913646698\n",
      "\tlearning rate:    0.01\n",
      "epoch 66\n",
      "\ttraining loss:    0.3595622837787769\n",
      "\tvalidation loss : 0.3138216435909271\n",
      "\tlearning rate:    0.01\n",
      "epoch 67\n",
      "\ttraining loss:    0.3673986714394366\n",
      "\tvalidation loss : 0.30861541628837585\n",
      "\tlearning rate:    0.01\n",
      "epoch 68\n",
      "\ttraining loss:    0.36667377636080883\n",
      "\tvalidation loss : 0.3120446503162384\n",
      "\tlearning rate:    0.01\n",
      "epoch 69\n",
      "\ttraining loss:    0.36354481343363154\n",
      "\tvalidation loss : 0.3230304419994354\n",
      "\tlearning rate:    0.01\n",
      "epoch 70\n",
      "\ttraining loss:    0.3571036043958586\n",
      "\tvalidation loss : 0.29591605067253113\n",
      "\tlearning rate:    0.01\n",
      "epoch 71\n",
      "\ttraining loss:    0.36592869448368665\n",
      "\tvalidation loss : 0.3019833266735077\n",
      "\tlearning rate:    0.01\n",
      "epoch 72\n",
      "\ttraining loss:    0.3602290869247718\n",
      "\tvalidation loss : 0.34744855761528015\n",
      "\tlearning rate:    0.01\n",
      "epoch 73\n",
      "\ttraining loss:    0.3595713067372314\n",
      "\tvalidation loss : 0.31320920586586\n",
      "\tlearning rate:    0.01\n",
      "epoch 74\n",
      "\ttraining loss:    0.36017128501514917\n",
      "\tvalidation loss : 0.3044659197330475\n",
      "\tlearning rate:    0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75\n",
      "\ttraining loss:    0.35938439628139873\n",
      "\tvalidation loss : 0.3040830194950104\n",
      "\tlearning rate:    0.01\n",
      "epoch 76\n",
      "\ttraining loss:    0.3593190956066866\n",
      "\tvalidation loss : 0.3040047287940979\n",
      "\tlearning rate:    0.01\n",
      "epoch 77\n",
      "\ttraining loss:    0.3589069835353093\n",
      "\tvalidation loss : 0.3040604293346405\n",
      "\tlearning rate:    0.01\n",
      "epoch 78\n",
      "\ttraining loss:    0.36145676403749183\n",
      "\tvalidation loss : 0.3215523362159729\n",
      "\tlearning rate:    0.01\n",
      "epoch 79\n",
      "\ttraining loss:    0.36817756710482424\n",
      "\tvalidation loss : 0.3055402636528015\n",
      "\tlearning rate:    0.01\n",
      "epoch 80\n",
      "\ttraining loss:    0.3591860151681744\n",
      "\tvalidation loss : 0.3107258975505829\n",
      "\tlearning rate:    0.01\n",
      "epoch 81\n",
      "\ttraining loss:    0.36963503013868804\n",
      "\tvalidation loss : 0.3087534010410309\n",
      "\tlearning rate:    0.01\n",
      "epoch 82\n",
      "\ttraining loss:    0.36118517901565206\n",
      "\tvalidation loss : 0.2937862277030945\n",
      "\tlearning rate:    0.01\n",
      "epoch 83\n",
      "\ttraining loss:    0.3632639834016073\n",
      "\tvalidation loss : 0.31790754199028015\n",
      "\tlearning rate:    0.01\n",
      "epoch 84\n",
      "\ttraining loss:    0.3615898588886026\n",
      "\tvalidation loss : 0.29796865582466125\n",
      "\tlearning rate:    0.01\n",
      "epoch 85\n",
      "\ttraining loss:    0.3596620069908314\n",
      "\tvalidation loss : 0.3063969016075134\n",
      "\tlearning rate:    0.01\n",
      "epoch 86\n",
      "\ttraining loss:    0.3641251258429934\n",
      "\tvalidation loss : 0.30848264694213867\n",
      "\tlearning rate:    0.01\n",
      "epoch 87\n",
      "\ttraining loss:    0.36239001863315456\n",
      "\tvalidation loss : 0.32140278816223145\n",
      "\tlearning rate:    0.01\n",
      "epoch 88\n",
      "\ttraining loss:    0.3606279006258386\n",
      "\tvalidation loss : 0.3072223961353302\n",
      "\tlearning rate:    0.01\n",
      "epoch 89\n",
      "\ttraining loss:    0.3688084424519148\n",
      "\tvalidation loss : 0.30590519309043884\n",
      "\tlearning rate:    0.01\n",
      "epoch 90\n",
      "\ttraining loss:    0.35930108388916393\n",
      "\tvalidation loss : 0.3240263760089874\n",
      "\tlearning rate:    0.01\n",
      "epoch 91\n",
      "\ttraining loss:    0.36494296911309976\n",
      "\tvalidation loss : 0.31727808713912964\n",
      "\tlearning rate:    0.01\n",
      "epoch 92\n",
      "\ttraining loss:    0.35892553945056727\n",
      "\tvalidation loss : 0.2981266677379608\n",
      "\tlearning rate:    0.01\n",
      "epoch 93\n",
      "\ttraining loss:    0.3611119076853893\n",
      "\tvalidation loss : 0.3153403401374817\n",
      "\tlearning rate:    0.01\n",
      "epoch 94\n",
      "\ttraining loss:    0.3599246278649471\n",
      "\tvalidation loss : 0.31488293409347534\n",
      "\tlearning rate:    0.01\n",
      "epoch 95\n",
      "\ttraining loss:    0.36155229904612557\n",
      "\tvalidation loss : 0.34028640389442444\n",
      "\tlearning rate:    0.01\n",
      "epoch 96\n",
      "\ttraining loss:    0.3647832890025905\n",
      "\tvalidation loss : 0.31973034143447876\n",
      "\tlearning rate:    0.01\n",
      "epoch 97\n",
      "\ttraining loss:    0.36613161971823116\n",
      "\tvalidation loss : 0.3108932673931122\n",
      "\tlearning rate:    0.01\n",
      "epoch 98\n",
      "\ttraining loss:    0.35577911101892346\n",
      "\tvalidation loss : 0.29504749178886414\n",
      "\tlearning rate:    0.01\n",
      "epoch 99\n",
      "\ttraining loss:    0.3681407525646882\n",
      "\tvalidation loss : 0.30555981397628784\n",
      "\tlearning rate:    0.01\n",
      "epoch 100\n",
      "\ttraining loss:    0.3617175442517781\n",
      "\tvalidation loss : 0.30689436197280884\n",
      "\tlearning rate:    0.01\n",
      "epoch 101\n",
      "\ttraining loss:    0.3587995752936504\n",
      "\tvalidation loss : 0.32192566990852356\n",
      "\tlearning rate:    0.01\n",
      "epoch 102\n",
      "\ttraining loss:    0.35957911317465735\n",
      "\tvalidation loss : 0.32741591334342957\n",
      "\tlearning rate:    0.01\n",
      "\n",
      "EARLY STOP\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "hidden_layer_size = 256\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "log_epochs = 1\n",
    "\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = Model()\n",
    "model.train(\n",
    "    train_dataloader, val_dataloader=val_dataloader,\n",
    "    epochs=epochs, lr=learning_rate, log_epochs=log_epochs,\n",
    "    patience=int(epochs*0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66fda72",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a65a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joaomacedo/opt/anaconda3/envs/oncase/lib/python3.7/site-packages/ipykernel_launcher.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8081395348837209,\n",
       " 'recall': 0.8633540372670807,\n",
       " 'f1_score': 0.8348348348348347}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.test(test_dataloader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f80cd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/classification/min_max_scaler']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join('..', 'models', 'classification')\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "model.save(path)\n",
    "\n",
    "with open(os.path.join(path, 'results.json'), 'w') as f:\n",
    "    f.write(json.dumps(results, indent=4, ensure_ascii=False))\n",
    "\n",
    "with open(os.path.join(path, 'train_info.json'), 'w') as f:\n",
    "    f.write(json.dumps({\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'hidden_layer_size': hidden_layer_size\n",
    "    }))\n",
    "    \n",
    "joblib.dump(scaler, os.path.join(path, 'min_max_scaler'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa246b",
   "metadata": {},
   "source": [
    "# Feature impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b864c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dataset.drop('target', axis = 1)\n",
    "Y = train_dataset['target']\n",
    "\n",
    "most_important_feature = SelectKBest(chi2, k=1).fit_transform(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a9e672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important feature: x2\n"
     ]
    }
   ],
   "source": [
    "for k in X.keys():\n",
    "    if most_important_feature[:, 0].tolist() == X[k].values.tolist():\n",
    "        print(f'Most important feature: {k}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892fcbe",
   "metadata": {},
   "source": [
    "A funcÃ£o SelectKBest roda uma analise estatisticas univariada para definir quais variaveis tem um maior impacto no target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32ca5307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f8103f49890>,\n",
       "  <matplotlib.axis.XTick at 0x7f8103f49650>],\n",
       " [Text(0, 0, 'x1'), Text(1, 0, 'x2')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=100)\n",
    "gb.fit(X, Y)\n",
    "\n",
    "plt.bar(range(X.shape[1]), gb.feature_importances_)\n",
    "plt.xticks(range(X.shape[1]), ['x1','x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa1e46",
   "metadata": {},
   "source": [
    "Utilizando o modelo GradientBoostingClassifier podemos ver que ele tambÃ©m aponta que a variavel de maior importancia Ã© a variavel x2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
